{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758412c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U peft transformers accelerate datasets bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f085ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0+cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling torch-2.4.0+cu118:\n",
      "  Successfully uninstalled torch-2.4.0+cu118\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp310-cp310-win_amd64.whl (2692.5 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\incha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.4.0+cu118\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall torch -y\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8d591b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f9466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since hendzh/PromptShield couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\incha\\.cache\\huggingface\\datasets\\hendzh___prompt_shield\\default\\0.0.0\\a5234cb1f5cdb256600cab64b8c961195b5e8404 (last modified on Sat May 17 17:42:20 2025).\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hendzh/PromptShield\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b298944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.51.3\n",
      "Transformers path: c:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers path:\", transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f8246b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Collecting transformers==4.38.2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 130.7/130.7 kB 7.5 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers==4.38.2)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.2)\n",
      "  Using cached huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.38.2)\n",
      "  Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0 (from transformers==4.38.2)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.38.2)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.2)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers==4.38.2)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.38.2)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting colorama (from tqdm>=4.27->transformers==4.38.2)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.38.2)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.38.2)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.38.2)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.38.2)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.9/8.5 MB 29.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.0/8.5 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.1/8.5 MB 22.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.4/8.5 MB 21.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.7/8.5 MB 21.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.8/8.5 MB 20.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 22.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 20.2 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.0/12.9 MB 63.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.9/12.9 MB 37.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.8/12.9 MB 30.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.6/12.9 MB 27.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.6/12.9 MB 25.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.4/12.9 MB 25.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.5/12.9 MB 24.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.0/12.9 MB 23.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.9/12.9 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.9/12.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.9/12.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.5/66.5 kB ? eta 0:00:00\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 274.0/274.0 kB 17.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 308.9/308.9 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.1/2.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 23.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "   ---------------------------------------- 0.0/159.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 159.6/159.6 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.8/105.8 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "   ---------------------------------------- 0.0/194.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 194.4/194.4 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 70.4/70.4 kB 3.8 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.7/128.7 kB 3.8 MB/s eta 0:00:00\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, safetensors, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, colorama, charset-normalizer, certifi, tqdm, requests, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.20\n",
      "    Uninstalling urllib3-1.26.20:\n",
      "      Successfully uninstalled urllib3-1.26.20\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.4\n",
      "    Uninstalling safetensors-0.4.4:\n",
      "      Successfully uninstalled safetensors-0.4.4\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.7.24\n",
      "    Uninstalling regex-2024.7.24:\n",
      "      Successfully uninstalled regex-2024.7.24\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.5.0\n",
      "    Uninstalling fsspec-2024.5.0:\n",
      "      Successfully uninstalled fsspec-2024.5.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.15.4\n",
      "    Uninstalling filelock-3.15.4:\n",
      "      Successfully uninstalled filelock-3.15.4\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.6.2\n",
      "    Uninstalling certifi-2024.6.2:\n",
      "      Successfully uninstalled certifi-2024.6.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.4\n",
      "    Uninstalling tqdm-4.66.4:\n",
      "      Successfully uninstalled tqdm-4.66.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.31.2\n",
      "    Uninstalling huggingface-hub-0.31.2:\n",
      "      Successfully uninstalled huggingface-hub-0.31.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 colorama-0.4.6 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.31.2 idna-3.10 numpy-2.2.5 packaging-25.0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.38.2 typing-extensions-4.13.2 urllib3-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~afetensors'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~aml'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~harset_normalizer'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awsebcli 3.20.10 requires colorama<0.4.4,>=0.2.5, but you have colorama 0.4.6 which is incompatible.\n",
      "awsebcli 3.20.10 requires urllib3<2,>=1.26.5, but you have urllib3 2.4.0 which is incompatible.\n",
      "botocore 1.31.85 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.4.0 which is incompatible.\n",
      "chromadb 0.5.5 requires numpy<2.0.0,>=1.22.5, but you have numpy 2.2.5 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "faiss-cpu 1.8.0.post1 requires numpy<2.0,>=1.0, but you have numpy 2.2.5 which is incompatible.\n",
      "langchain 0.2.12 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.5 which is incompatible.\n",
      "langchain-chroma 0.1.2 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.5 which is incompatible.\n",
      "langchain-community 0.2.11 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.5 which is incompatible.\n",
      "langchain-core 0.2.28 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires transformers>=4.39.0, but you have transformers 4.38.2 which is incompatible.\n",
      "langchainhub 0.1.20 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.5 which is incompatible.\n",
      "onnxruntime 1.18.1 requires numpy<2.0,>=1.21.6, but you have numpy 2.2.5 which is incompatible.\n",
      "s3transfer 0.10.2 requires botocore<2.0a.0,>=1.33.2, but you have botocore 1.31.85 which is incompatible.\n",
      "streamlit 1.36.0 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.5 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall transformers -y\n",
    "%pip install transformers==4.38.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b6396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b90a3f9494933ae64e9e06b9ccf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09aed62eda6c4856b9aad06bbaddc848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdac7ba0f1004536afbbc596a04e45e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"prompt\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "train_dataset = tokenized[\"train\"].with_format(\"torch\")\n",
    "val_dataset = tokenized[\"validation\"].with_format(\"torch\")\n",
    "\n",
    "# ✅ Load base model with quantization (4-bit)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b5a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8f8bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = prepare_model_for_kbit_training(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf17143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9050cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # Correct target modules for BERT-base\n",
    "    target_modules=[\n",
    "        \"bert.encoder.layer.0.attention.self.query\",\n",
    "        \"bert.encoder.layer.0.attention.self.key\",\n",
    "        \"bert.encoder.layer.0.attention.self.value\",\n",
    "        \"bert.encoder.layer.0.attention.output.dense\",\n",
    "        \"bert.encoder.layer.0.intermediate.dense\",\n",
    "        \"bert.encoder.layer.0.output.dense\",\n",
    "        # Add more layers as needed\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2ada7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 112,130 || all params: 109,595,908 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "339136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"./qlora-bert-base\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed7f77",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[1;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./qlora-bert-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-bert-base\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=2e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc18f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c2d7654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\incha\\AppData\\Local\\Temp\\ipykernel_21372\\2721217396.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "059aa50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:401: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:354: UserWarning: FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         load_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2244\u001b[0m         \u001b[38;5;66;03m# release memory\u001b[39;00m\n\u001b[1;32m-> 2245\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n\u001b[0;32m   2246\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_issue_warnings_after_load(load_result)\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;66;03m# Load adapters following PR # 24096\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2555\u001b[0m             shutil.rmtree(staging_output_dir)\n\u001b[0;32m   2557\u001b[0m     self.args.distributed_state.wait_for_everyone()\n\u001b[0;32m   2559\u001b[0m def _save_rng_state(self, output_dir):\n\u001b[1;32m-> 2560\u001b[0m     # Save RNG state in non-distributed training\n\u001b[0;32m   2561\u001b[0m     rng_states = {\n\u001b[0;32m   2562\u001b[0m         \"python\": random.getstate(),\n\u001b[0;32m   2563\u001b[0m         \"numpy\": np.random.get_state(),\n\u001b[0;32m   2564\u001b[0m         \"cpu\": torch.random.get_rng_state(),\n\u001b[0;32m   2565\u001b[0m     }\n\u001b[0;32m   2566\u001b[0m     if torch.cuda.is_available():\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3736\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3710\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3711\u001b[0m \u001b[38;5;124;03mCreates a draft of a model card using the information available to the `Trainer`.\u001b[39;00m\n\u001b[0;32m   3712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3733\u001b[0m \u001b[38;5;124;03m       One or several dataset arguments, to be included in the metadata of the model card.\u001b[39;00m\n\u001b[0;32m   3734\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_world_process_zero():\n\u001b[1;32m-> 3736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   3738\u001b[0m model_card_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3739\u001b[0m is_peft_library \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3801\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3797\u001b[0m     commit_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining in progress, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3799\u001b[0m model_push_job \u001b[38;5;241m=\u001b[39m upload_folder(\n\u001b[0;32m   3800\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id,\n\u001b[1;32m-> 3801\u001b[0m     folder_path\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m   3802\u001b[0m     commit_message\u001b[38;5;241m=\u001b[39mcommit_message,\n\u001b[0;32m   3803\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token,\n\u001b[0;32m   3804\u001b[0m     run_as_future\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   3805\u001b[0m     ignore_patterns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3806\u001b[0m )\n\u001b[0;32m   3808\u001b[0m push_jobs \u001b[38;5;241m=\u001b[39m [model_push_job]\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_strategy \u001b[38;5;129;01min\u001b[39;00m [HubStrategy\u001b[38;5;241m.\u001b[39mCHECKPOINT, HubStrategy\u001b[38;5;241m.\u001b[39mALL_CHECKPOINTS]:\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\peft_model.py:1559\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1558\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1559\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1560\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1561\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1562\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1563\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1564\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1565\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1566\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1567\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1568\u001b[0m         )\n\u001b[0;32m   1570\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:987\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;124;03m    Resizes position embeddings of the model if `new_num_position_embeddings != config.max_position_embeddings`.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m            the size will remove vectors from the end.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert\u001b[38;5;241m.\u001b[39mresize_position_embeddings(new_num_position_embeddings)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(DISTILBERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m    979\u001b[0m     checkpoint\u001b[38;5;241m=\u001b[39m_CHECKPOINT_FOR_DOC,\n\u001b[0;32m    980\u001b[0m     output_type\u001b[38;5;241m=\u001b[39mSequenceClassifierOutput,\n\u001b[0;32m    981\u001b[0m     config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    985\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    986\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m--> 987\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    988\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    989\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    990\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    991\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    992\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SequenceClassifierOutput, Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]:\n\u001b[0;32m    994\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\other.py:399\u001b[0m, in \u001b[0;36mAuxiliaryTrainingWrapper.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_wrapped_disabled(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_wrapped(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_batch_forward(x, \u001b[38;5;241m*\u001b[39margs, adapter_names\u001b[38;5;241m=\u001b[39madapter_names, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\utils\\other.py:465\u001b[0m, in \u001b[0;36mModulesToSaveWrapper._forward_wrapped\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters[\u001b[38;5;241m0\u001b[39m]](x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 468\u001b[0m     \u001b[43mfix_4bit_weight_quant_state_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# weights are cast automatically as Int8Params, but the bias has to be cast manually\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:360\u001b[0m, in \u001b[0;36mfix_4bit_weight_quant_state_from_module\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    354\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    356\u001b[0m     )\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# the quant state got lost when the parameter got converted. This happens for example for fsdp\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# since we registered the module, we can recover the state here\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module\u001b[38;5;241m.\u001b[39mweight, Params4bit):\n\u001b[0;32m    362\u001b[0m     module\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Params4bit(module\u001b[38;5;241m.\u001b[39mweight, quant_storage\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mquant_storage, bnb_quantized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qlora-distilbert\")\n",
    "tokenizer.save_pretrained(\"./qlora-distilbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82f4120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a89bd8bd8649749fa1bd29590edf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1434b634b47c470ea282e6dc1e33e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODULES IN BERT MODEL ===\n",
      "Attention module: bert.encoder.layer.0.attention\n",
      "Attention module: bert.encoder.layer.0.attention.self\n",
      "Linear module: bert.encoder.layer.0.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.0.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.0.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.0.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.0.attention.output\n",
      "Linear module: bert.encoder.layer.0.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.0.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.0.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.0.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.0.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.1.attention\n",
      "Attention module: bert.encoder.layer.1.attention.self\n",
      "Linear module: bert.encoder.layer.1.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.1.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.1.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.1.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.1.attention.output\n",
      "Linear module: bert.encoder.layer.1.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.1.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.1.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.1.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.1.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.2.attention\n",
      "Attention module: bert.encoder.layer.2.attention.self\n",
      "Linear module: bert.encoder.layer.2.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.2.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.2.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.2.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.2.attention.output\n",
      "Linear module: bert.encoder.layer.2.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.2.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.2.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.2.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.2.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.3.attention\n",
      "Attention module: bert.encoder.layer.3.attention.self\n",
      "Linear module: bert.encoder.layer.3.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.3.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.3.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.3.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.3.attention.output\n",
      "Linear module: bert.encoder.layer.3.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.3.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.3.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.3.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.3.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.4.attention\n",
      "Attention module: bert.encoder.layer.4.attention.self\n",
      "Linear module: bert.encoder.layer.4.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.4.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.4.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.4.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.4.attention.output\n",
      "Linear module: bert.encoder.layer.4.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.4.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.4.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.4.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.4.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.5.attention\n",
      "Attention module: bert.encoder.layer.5.attention.self\n",
      "Linear module: bert.encoder.layer.5.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.5.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.5.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.5.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.5.attention.output\n",
      "Linear module: bert.encoder.layer.5.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.5.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.5.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.5.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.5.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.6.attention\n",
      "Attention module: bert.encoder.layer.6.attention.self\n",
      "Linear module: bert.encoder.layer.6.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.6.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.6.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.6.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.6.attention.output\n",
      "Linear module: bert.encoder.layer.6.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.6.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.6.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.6.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.6.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.7.attention\n",
      "Attention module: bert.encoder.layer.7.attention.self\n",
      "Linear module: bert.encoder.layer.7.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.7.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.7.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.7.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.7.attention.output\n",
      "Linear module: bert.encoder.layer.7.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.7.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.7.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.7.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.7.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.8.attention\n",
      "Attention module: bert.encoder.layer.8.attention.self\n",
      "Linear module: bert.encoder.layer.8.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.8.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.8.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.8.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.8.attention.output\n",
      "Linear module: bert.encoder.layer.8.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.8.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.8.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.8.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.8.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.9.attention\n",
      "Attention module: bert.encoder.layer.9.attention.self\n",
      "Linear module: bert.encoder.layer.9.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.9.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.9.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.9.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.9.attention.output\n",
      "Linear module: bert.encoder.layer.9.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.9.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.9.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.9.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.9.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.10.attention\n",
      "Attention module: bert.encoder.layer.10.attention.self\n",
      "Linear module: bert.encoder.layer.10.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.10.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.10.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.10.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.10.attention.output\n",
      "Linear module: bert.encoder.layer.10.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.10.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.10.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.10.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.10.output.dense, shape: torch.Size([1179648, 1])\n",
      "Attention module: bert.encoder.layer.11.attention\n",
      "Attention module: bert.encoder.layer.11.attention.self\n",
      "Linear module: bert.encoder.layer.11.attention.self.query, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.11.attention.self.key, shape: torch.Size([294912, 1])\n",
      "Linear module: bert.encoder.layer.11.attention.self.value, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.11.attention.self.dropout\n",
      "Attention module: bert.encoder.layer.11.attention.output\n",
      "Linear module: bert.encoder.layer.11.attention.output.dense, shape: torch.Size([294912, 1])\n",
      "Attention module: bert.encoder.layer.11.attention.output.LayerNorm\n",
      "Attention module: bert.encoder.layer.11.attention.output.dropout\n",
      "Linear module: bert.encoder.layer.11.intermediate.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.encoder.layer.11.output.dense, shape: torch.Size([1179648, 1])\n",
      "Linear module: bert.pooler.dense, shape: torch.Size([294912, 1])\n",
      "Linear module: classifier, shape: torch.Size([2, 768])\n",
      "\n",
      "=== RECOMMENDED TARGET MODULES ===\n",
      "Potential target modules (Linear layers): ['bert.encoder.layer.0.attention.self.query', 'bert.encoder.layer.0.attention.self.key', 'bert.encoder.layer.0.attention.self.value', 'bert.encoder.layer.0.attention.output.dense', 'bert.encoder.layer.0.intermediate.dense', 'bert.encoder.layer.0.output.dense', 'bert.encoder.layer.1.attention.self.query', 'bert.encoder.layer.1.attention.self.key', 'bert.encoder.layer.1.attention.self.value', 'bert.encoder.layer.1.attention.output.dense', 'bert.encoder.layer.1.intermediate.dense', 'bert.encoder.layer.1.output.dense', 'bert.encoder.layer.2.attention.self.query', 'bert.encoder.layer.2.attention.self.key', 'bert.encoder.layer.2.attention.self.value', 'bert.encoder.layer.2.attention.output.dense', 'bert.encoder.layer.2.intermediate.dense', 'bert.encoder.layer.2.output.dense', 'bert.encoder.layer.3.attention.self.query', 'bert.encoder.layer.3.attention.self.key', 'bert.encoder.layer.3.attention.self.value', 'bert.encoder.layer.3.attention.output.dense', 'bert.encoder.layer.3.intermediate.dense', 'bert.encoder.layer.3.output.dense', 'bert.encoder.layer.4.attention.self.query', 'bert.encoder.layer.4.attention.self.key', 'bert.encoder.layer.4.attention.self.value', 'bert.encoder.layer.4.attention.output.dense', 'bert.encoder.layer.4.intermediate.dense', 'bert.encoder.layer.4.output.dense', 'bert.encoder.layer.5.attention.self.query', 'bert.encoder.layer.5.attention.self.key', 'bert.encoder.layer.5.attention.self.value', 'bert.encoder.layer.5.attention.output.dense', 'bert.encoder.layer.5.intermediate.dense', 'bert.encoder.layer.5.output.dense', 'bert.encoder.layer.6.attention.self.query', 'bert.encoder.layer.6.attention.self.key', 'bert.encoder.layer.6.attention.self.value', 'bert.encoder.layer.6.attention.output.dense', 'bert.encoder.layer.6.intermediate.dense', 'bert.encoder.layer.6.output.dense', 'bert.encoder.layer.7.attention.self.query', 'bert.encoder.layer.7.attention.self.key', 'bert.encoder.layer.7.attention.self.value', 'bert.encoder.layer.7.attention.output.dense', 'bert.encoder.layer.7.intermediate.dense', 'bert.encoder.layer.7.output.dense', 'bert.encoder.layer.8.attention.self.query', 'bert.encoder.layer.8.attention.self.key', 'bert.encoder.layer.8.attention.self.value', 'bert.encoder.layer.8.attention.output.dense', 'bert.encoder.layer.8.intermediate.dense', 'bert.encoder.layer.8.output.dense', 'bert.encoder.layer.9.attention.self.query', 'bert.encoder.layer.9.attention.self.key', 'bert.encoder.layer.9.attention.self.value', 'bert.encoder.layer.9.attention.output.dense', 'bert.encoder.layer.9.intermediate.dense', 'bert.encoder.layer.9.output.dense', 'bert.encoder.layer.10.attention.self.query', 'bert.encoder.layer.10.attention.self.key', 'bert.encoder.layer.10.attention.self.value', 'bert.encoder.layer.10.attention.output.dense', 'bert.encoder.layer.10.intermediate.dense', 'bert.encoder.layer.10.output.dense', 'bert.encoder.layer.11.attention.self.query', 'bert.encoder.layer.11.attention.self.key', 'bert.encoder.layer.11.attention.self.value', 'bert.encoder.layer.11.attention.output.dense', 'bert.encoder.layer.11.intermediate.dense', 'bert.encoder.layer.11.output.dense', 'bert.pooler.dense', 'classifier']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Load the model with the same configuration\n",
    "model_name = \"bert-base-uncased\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Print all module names to identify the correct target modules\n",
    "print(\"=== MODULES IN BERT MODEL ===\")\n",
    "for name, module in base_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(f\"Linear module: {name}, shape: {module.weight.shape}\")\n",
    "    # Also print attention modules\n",
    "    elif \"attention\" in name.lower():\n",
    "        print(f\"Attention module: {name}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDED TARGET MODULES ===\")\n",
    "# Find potential modules that could be targeted for LoRA\n",
    "linear_modules = [name for name, module in base_model.named_modules() \n",
    "                 if isinstance(module, torch.nn.Linear)]\n",
    "print(\"Potential target modules (Linear layers):\", linear_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca895472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
